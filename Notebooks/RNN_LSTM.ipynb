{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import glob\n",
    "import pathlib\n",
    "import darts\n",
    "from darts import TimeSeries\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helping Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    # Convert inputs to numpy arrays for easier calculations\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate individual metrics\n",
    "    mae = np.mean(np.abs(predicted - actual))\n",
    "    rmse = np.sqrt(np.mean((predicted - actual) ** 2))\n",
    "    mape = np.mean(np.abs((predicted - actual) / actual)) * 100\n",
    "    mse = np.mean((predicted - actual) ** 2)\n",
    "    \n",
    "    metrics = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'MSE': mse\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Time Store_Number % family\n",
    "## Records\n",
    "* Store Number : 1 & Family : Automative \n",
    "* Store Number : 2 & Family :Automative\n",
    "* Store Number : 3 & Family :Automative\n",
    "* Store Number : 7 & Family :Automative\n",
    "* Store Number : 1 & Family : Seafood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = '2_AUTOMOTIVE'\n",
    "df = pd.read_csv(f'../ProcessedData/GroupData/{fileName}.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['date','sales']]\n",
    "df = df.drop_duplicates()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Data into Training & Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame containing daily data\n",
    "series = TimeSeries.from_dataframe(df, \"date\", \"sales\", freq='1D', fill_missing_dates=True, fillna_value=0)\n",
    "\n",
    "\n",
    "split_point = 0.80\n",
    "\n",
    "train_series, test_series = series.split_after(split_point)\n",
    "\n",
    "# Normalize the time series (note: we avoid fitting the transformer on the validation set)\n",
    "transformer = Scaler()\n",
    "train_transformed = transformer.fit_transform(train_series)\n",
    "test_transformed = transformer.transform(test_series)\n",
    "series_transformed = transformer.transform(series)\n",
    "\n",
    "\n",
    "# Set the figure size and style\n",
    "plt.figure(figsize=(18, 6))\n",
    "# Plot the training and testing data\n",
    "train_transformed.plot(label='Training Data', color='blue', linewidth=1.5, marker='o')\n",
    "test_transformed.plot(label='Testing Data', color='orange', linewidth=1.5, marker='o')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Training and Testing Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "\n",
    "# Add grid lines\n",
    "plt.grid(True)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models.forecasting.rnn_model import  RNNModel\n",
    "\n",
    "\n",
    "rnn_model =  RNNModel(\n",
    "    model=\"LSTM\",\n",
    "    hidden_dim=20,\n",
    "    dropout=0,\n",
    "    batch_size=16,\n",
    "    n_epochs=300,\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    model_name=\"StoreNBR\",\n",
    "    log_tensorboard=True,\n",
    "    random_state=42,\n",
    "    training_length=20,\n",
    "    input_chunk_length=14,\n",
    "    force_reset=True,\n",
    "    save_checkpoints=True,\n",
    ")\n",
    "rnn_model.fit(train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to predict and evaluate\n",
    "def predict_and_evaluate(window_size, prediction_horizon, slide_step, test_series, model,result_plot_path,transformer ):\n",
    "    num_predictions = len(test_series) - window_size - prediction_horizon + 1\n",
    "    \n",
    "    meta_information_evaluation = {\n",
    "        'Iterations': [],\n",
    "        'MAE': [],\n",
    "        'RMSE': [],\n",
    "        'MAPE': [],\n",
    "        'MSE': [],\n",
    "        'input_window_size': [],\n",
    "        'horizon': [],\n",
    "        'stride': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        for i in tqdm(range(0, num_predictions, slide_step)):\n",
    "            input_window = test_series[i:i + window_size]\n",
    "            ground_truth = test_series[i + window_size:i + window_size + prediction_horizon]\n",
    "            forecast = model.predict(n=prediction_horizon, series=input_window)\n",
    "            \n",
    "            input_window = transformer.inverse_transform(input_window)           \n",
    "            ground_truth = transformer.inverse_transform(ground_truth)\n",
    "            forecast = transformer.inverse_transform(forecast)\n",
    "            # print(ground_truth)\n",
    "\n",
    "            # print('1:Actual:', ground_truth.values().flatten().tolist())\n",
    "            # print('2:Predicted:', predicted.values().flatten().tolist())\n",
    "            # print('3:Input:',input_window.values().flatten().tolist())\n",
    "\n",
    "            actual = ground_truth.values().flatten().tolist()\n",
    "            predicted = forecast.values().flatten().tolist()\n",
    "            \n",
    "            metrics = calculate_metrics(actual, predicted)\n",
    "            \n",
    "            meta_information_evaluation['Iterations'].append(i)\n",
    "            meta_information_evaluation['MAE'].append(metrics['MAE'])\n",
    "            meta_information_evaluation['RMSE'].append(metrics['RMSE'])\n",
    "            meta_information_evaluation['MAPE'].append(metrics['MAPE'])\n",
    "            meta_information_evaluation['MSE'].append(metrics['MSE'])\n",
    "            meta_information_evaluation['input_window_size'].append(window_size)\n",
    "            meta_information_evaluation['horizon'].append(prediction_horizon)\n",
    "            meta_information_evaluation['stride'].append(slide_step)\n",
    "            \n",
    "            bypass_information = {\n",
    "                'slide_step':slide_step,\n",
    "                'window_size':window_size,\n",
    "                'horizon':prediction_horizon,            \n",
    "            }\n",
    "            create_plots(input_window,forecast,ground_truth,result_plot_path,bypass_information)\n",
    "\n",
    "        evalaution_df = pd.DataFrame.from_dict(meta_information_evaluation)\n",
    "        \n",
    "        return evalaution_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error Occurred in fuction predict_and_evaluate():', e)\n",
    "        evalaution_df = pd.DataFrame.from_dict(meta_information_evaluation)\n",
    "        \n",
    "        return evalaution_df\n",
    "\n",
    "# Function to create plots\n",
    "def create_plots(input_window, forecast, ground_truth,result_plot_path,bypass_information):\n",
    "    \n",
    "    plt.figure(figsize=(30, 6))\n",
    "    input_window.plot(label='Input Data', marker='o')\n",
    "    forecast.plot(label='Predicted', marker='o')\n",
    "    ground_truth.plot(label='Ground Truth', marker='o')\n",
    "    \n",
    "    combined_time_index = input_window.time_index.append(forecast.time_index).append(ground_truth.time_index)\n",
    "    starting_date_of_input_data = input_window.time_index[0].strftime(\"%Y-%m-%d\")\n",
    "    ending_date_of_input_data = input_window.time_index[-1].strftime(\"%Y-%m-%d\")\n",
    "    starting_date_predicted = forecast.time_index[0].strftime(\"%Y-%m-%d\")\n",
    "    ending_date_of_predicted = forecast.time_index[-1].strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    plt.xticks(combined_time_index, combined_time_index.strftime('%Y-%m-%d'), rotation=90)\n",
    "    plt.title(f'Results of Input Data from {starting_date_of_input_data} to {ending_date_of_input_data} & Evaluation on from {starting_date_predicted} to {ending_date_of_predicted}', fontsize=16)\n",
    "    plt.ylabel('Quantity Sold', fontsize=14)\n",
    "    plt.xlabel('Dates', fontsize=14)\n",
    "    plt.legend()\n",
    "    \n",
    "    plot_filename = f\"{result_plot_path}/{bypass_information['window_size']}_{bypass_information['horizon']}_{bypass_information['slide_step']}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    # plt.close()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model_name,model_object,test_series,transformer,FileName):\n",
    "    \n",
    "    result_path = f'../ProcessedData/Results/{model_name}/{FileName}'\n",
    "    result_plot_path = f'../ProcessedData/Results/{model_name}/{FileName}/{model_name}_Plots'\n",
    "    os.makedirs(result_path,exist_ok=True)\n",
    "    os.makedirs(result_plot_path,exist_ok=True)\n",
    "\n",
    "        # Set your parameters\n",
    "    window_sizes = [30, 45, 90]\n",
    "    prediction_horizons = [15, 30,35]\n",
    "    slide_steps = [5, 10, 15]\n",
    "\n",
    "    test_series = test_series\n",
    "    model = model_object\n",
    "\n",
    "    for window_size in window_sizes:\n",
    "        for prediction_horizon in prediction_horizons:\n",
    "            for slide_step in slide_steps:\n",
    "                print(f'Iteration : Window size : {window_size} Horizan: {prediction_horizon}, Stride : {slide_step}')\n",
    "                evaluation_df = predict_and_evaluate(window_size, prediction_horizon, slide_step, test_series, model,result_plot_path,transformer)\n",
    "                evaluation_df.to_csv(f'{result_path}/window_size_{window_size}_horizon_{prediction_horizon}_stride_{slide_step}.csv', index=False)\n",
    "                \n",
    "                print(f'Window_size_{window_size}_prediction_horizon_{prediction_horizon}_slide_step_{slide_step} - Evaluation completed.')\n",
    "        #         break\n",
    "        #     break\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RNN_LSTM'\n",
    "FileName = fileName\n",
    "model_object = rnn_model\n",
    "test_series = test_transformed \n",
    "model_evaluation(model_name,model_object,test_series,transformer,FileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def aggregate_evaluation_results(file_pattern):\n",
    "    eval_dict = {\n",
    "        'window_size': [],\n",
    "        'horizan': [],\n",
    "        'stride': [],\n",
    "        'AVG_MAE': [],\n",
    "        'AVG_MSE': [],\n",
    "        'AVG_RMSE': [],\n",
    "        'AVG_MAPE': [],\n",
    "    }\n",
    "    \n",
    "    paths = glob.glob(file_pattern)\n",
    "    \n",
    "    for path in paths:\n",
    "        window_size = path.split('/')[-1].split('_')[2]\n",
    "        horizan = path.split('/')[-1].split('_')[4]\n",
    "        stride = path.split('/')[-1].split('_')[6].split('.')[0]\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "        eval_dict['window_size'].append(window_size)\n",
    "        eval_dict['horizan'].append(horizan)\n",
    "        eval_dict['stride'].append(stride)\n",
    "\n",
    "        eval_dict['AVG_MAE'].append(df['MAE'].mean())\n",
    "        eval_dict['AVG_MSE'].append(df['MSE'].mean())\n",
    "        eval_dict['AVG_RMSE'].append(df['RMSE'].mean())\n",
    "        eval_dict['AVG_MAPE'].append(df['MAPE'].mean())\n",
    "    \n",
    "    eval_df = pd.DataFrame.from_dict(eval_dict)\n",
    "    eval_df = eval_df.dropna()\n",
    "    eval_df.sort_values(['window_size', 'horizan', 'stride'], inplace=True, ascending=True)\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "# Example usage\n",
    "file_pattern = '../ProcessedData/Results/RNN_LSTM/*.csv'\n",
    "result_df = aggregate_evaluation_results(file_pattern)\n",
    "result_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
